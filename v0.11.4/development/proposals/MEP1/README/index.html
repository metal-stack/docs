<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distributed Metal Control Plane · metal-stack</title><script data-outdated-warner src="../../../../assets/warner.js"></script><link rel="canonical" href="https://docs.metal-stack.io/development/proposals/MEP1/README/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../../assets/documenter.js"></script><script src="../../../../siteinfo.js"></script><script src="../../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../../assets/themeswap.js"></script><link href="../../../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../../../assets/youtube.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../../"><img src="../../../../assets/logo.svg" alt="metal-stack logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../../">metal-stack</a></span></div><form class="docs-search" action="../../../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../../../">Introduction</a></li><li><span class="tocitem">Overview</span><ul><li><a class="tocitem" href="../../../../overview/architecture/">Architecture</a></li><li><a class="tocitem" href="../../../../overview/networking/">Networking</a></li><li><a class="tocitem" href="../../../../overview/hardware/">Hardware Support</a></li><li><a class="tocitem" href="../../../../overview/os/">Operating Systems</a></li><li><a class="tocitem" href="../../../../overview/kubernetes/">Kubernetes Integration</a></li><li><a class="tocitem" href="../../../../overview/storage/">Storage</a></li><li><a class="tocitem" href="../../../../overview/comparison/">Comparison</a></li></ul></li><li><a class="tocitem" href="../../../../quickstart/">Quickstart</a></li><li><span class="tocitem">Installation &amp; Administration</span><ul><li><a class="tocitem" href="../../../../installation/deployment/">Installation</a></li><li><a class="tocitem" href="../../../../installation/monitoring/">Monitoring</a></li><li><a class="tocitem" href="../../../../installation/troubleshoot/">Troubleshoot</a></li></ul></li><li><span class="tocitem">User Guides</span><ul><li><a class="tocitem" href="../../../../external/mini-lab/README/">mini-lab</a></li><li><a class="tocitem" href="../../../../external/metalctl/README/">metalctl</a></li><li><a class="tocitem" href="../../../../external/csi-driver-lvm/README/">csi-driver-lvm</a></li><li><a class="tocitem" href="../../../../external/firewall-controller/README/">firewall-controller</a></li></ul></li><li><a class="tocitem" href="../../../../apidocs/apidocs/">API Documentation</a></li><li><span class="tocitem">Development</span><ul><li><a class="tocitem" href="../../../client_libraries/">Client Libraries</a></li><li><a class="tocitem" href="../../../roadmap/">Roadmap</a></li><li><a class="tocitem" href="../../">Enhancement Proposals</a></li><li><a class="tocitem" href="../../../contributing/">Contributing</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Distributed Metal Control Plane</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distributed Metal Control Plane</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/metal-stack/docs/blob/master/docs/src/development/proposals/MEP1/README.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Distributed-Metal-Control-Plane"><a class="docs-heading-anchor" href="#Distributed-Metal-Control-Plane">Distributed Metal Control Plane</a><a id="Distributed-Metal-Control-Plane-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Metal-Control-Plane" title="Permalink"></a></h1><h2 id="Problem-Statement"><a class="docs-heading-anchor" href="#Problem-Statement">Problem Statement</a><a id="Problem-Statement-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-Statement" title="Permalink"></a></h2><p>We face the situation that we argue for running bare metal on premise because this way the customers can control where and how their software and data are processed and stored. On the other hand, we have currently decided that our metal-api control plane components run on a kubernetes cluster (in our case on a cluster provided by one of the available hyperscalers).</p><p>Running the control plane on Kubernetes has the following benefits:</p><ul><li>Ease of deployment</li><li>Get most, if not all, of the required infrastructure services like (probably incomplete):<ul><li>IPs</li><li>DNS</li><li>L7-Loadbalancing</li><li>Storage</li><li>S3 Backup</li><li>High Availability</li></ul></li></ul><p>Using a kubernetes as a service offering from one of the hyperscalers, enables us to focus on using kubernetes instead of maintaining it as well.</p><h2 id="Goal"><a class="docs-heading-anchor" href="#Goal">Goal</a><a id="Goal-1"></a><a class="docs-heading-anchor-permalink" href="#Goal" title="Permalink"></a></h2><p>It would be much saner if metal-stack has no, or only minimal dependencies to external services. Imagine a metal-stack deployment in a plant, it would be optimal if we only have to deliver a single rack with servers and networking gear installed and wired, plug that rack to the power supply and a internet uplink and its ready to go.</p><p>Have a second plant which you want to be part of all your plants? Just tell both that they are part of something bigger and metal-api knows of two partitions.</p><h2 id="Possible-Solutions"><a class="docs-heading-anchor" href="#Possible-Solutions">Possible Solutions</a><a id="Possible-Solutions-1"></a><a class="docs-heading-anchor-permalink" href="#Possible-Solutions" title="Permalink"></a></h2><p>We can think of two different solutions to this vision:</p><ol><li>Keep the central control plane approach and require some sort of kubernetes deployment accessible from the internet. This has the downside that the user must, provide a managed kubernetes deployment in his own datacenter or uses a hyperscaler. Still not optimal.</li><li>Install the metal-api and all its dependencies in every partition, replicate or shard the databases to every connected partition, make them know each other. Connect the partitions over the internet with some sort of vpn to make the services visible to each other.</li></ol><p>As we can see, the first approach does not really address the problem, therefore i will describe solution #2 in more details.</p><h2 id="Central/Current-setup"><a class="docs-heading-anchor" href="#Central/Current-setup">Central/Current setup</a><a id="Central/Current-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Central/Current-setup" title="Permalink"></a></h2><h3 id="Stateful-services"><a class="docs-heading-anchor" href="#Stateful-services">Stateful services</a><a id="Stateful-services-1"></a><a class="docs-heading-anchor-permalink" href="#Stateful-services" title="Permalink"></a></h3><p>Every distributed system suffer from handling state in a scalable, fast and correct way. To start how to cope with the state, we first must identify which state can be seen as partition local only and which state must be synchronous for read, and synchronous for writes across partitions.</p><p>Affected states:</p><ul><li>masterdata: e.g. tenant and project must be present in every partition, but these are entities which are read often but updates are rare. A write can therefore be visible with a decent delay in a distinct partition with no consequences.</li><li>ipam: the prefixes and ip´s allocated from machines. These entities are also read often and rare updates. But we must differentiate between dirty reads for different types. A machine network is partition local, ips acquired from such a network must by synchronous in the same partition. Ips acquired from global networks such as internet must by synchronous for all partitions, as otherwise a internet ip could be acquired twice.</li><li>vrf ids: they must only be unique in one partition</li><li>image and size configurations: read often, written seldom, so no high requirements on the storage of these entities.</li><li>images: os images are already replicated from a central s3 storage to a per partition s3 service. metal-hammer kernel and initrd are small and pull always from the central s3, can be done similar to os images.</li><li>machine and machine allocation: must be only synchronous in the partition</li><li>switch: must be only synchronous in the partition</li><li>nsq messages: do not need to cross partition boundaries. No need to keep the messages persistent, even the opposite is true, we don&#39;t want to have the messages persist for a longer period.</li></ul><p>Now we can see that the most critical state to held and synchronize are the IPAM data, because these entities must be guaranteed to be synchronously updated, while being updated frequently.</p><p>Datastores:</p><p>We use three different types of datastores to persist the states of the metal application.</p><ul><li>rethinkdb is the main datastore for almost all entities managed by metal-api</li><li>postgresql is used for masterdata and ipam data.</li><li>nsq uses disk and memory tho store the messages.</li></ul><h3 id="Stateless-services"><a class="docs-heading-anchor" href="#Stateless-services">Stateless services</a><a id="Stateless-services-1"></a><a class="docs-heading-anchor-permalink" href="#Stateless-services" title="Permalink"></a></h3><p>These are the easy part, all of our services which are stateless can be scaled up and down without any impact on functionality. Even the stateful services like masterdata and metal-api rely fully on the underlying datastore and can therefore also be scaled up and down to meet scalability requirements.</p><p>Albeit, most of these services need to be placed behind a loadbalancer which does the L4/L7 balancing across the started/available replicas of the service for the clients talking to it. This is actually provided by kubernetes with either service type loadbalancer or type clusterip.</p><p>One exception is the <code>metal-console</code> service which must have the partition in it´s dns name now, because there is no direct network connectivity between the management networks of the partitions. See &quot;Network Setup)</p><h2 id="Distributed-setup"><a class="docs-heading-anchor" href="#Distributed-setup">Distributed setup</a><a id="Distributed-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-setup" title="Permalink"></a></h2><h3 id="State"><a class="docs-heading-anchor" href="#State">State</a><a id="State-1"></a><a class="docs-heading-anchor-permalink" href="#State" title="Permalink"></a></h3><p>In order to replicate certain data which must be available across all partitions we can use on of the existing open source databases which enable such kind of setup. There are a few avaible out there, the following uncomplete list will highlight the pro´s and cons of each.</p><ul><li><p>RethinkDB</p><p>We already store most of our data in RethinkDB and it gives already the ability to synchronize the data in a distributed manner with different guarantees for consistency and latency. This is described here: <a href="https://rethinkdb.com/docs/sharding-and-replication/">Scaling, Sharding and replication</a>. But because rethinkdb has a rough history and unsure future with the last release took more than a year, we in the team already thought that we eventually must move away from rethinkdb in the future.</p></li><li><p>Postgresql</p><p>Postgres does not have a multi datacenter with replication in both directions, it just can make the remote instance store the same data.</p></li><li><p>CockroachDB</p><p>Is a Postgresql compatible database enginge on the wire. CockroachDB gives you both, ACID and geo replication with writes allowed from all connected members. It is even possible to configure <a href="https://www.cockroachlabs.com/docs/stable/topology-follow-the-workload.html">Follow the Workload</a> and <a href="https://www.cockroachlabs.com/docs/v19.2/topology-geo-partitioned-replicas.html#main-content">Geo Partitioning and Replication</a>.</p></li></ul><p>If we migrate all metal-api entities to be stored the same way we store masterdata, we could use cockroachdb to store all metal entities in one ore more databases spread across all partitions and still ensure consistency and high availability.</p><p>A simple setup how this would look like is shown here.</p><p><img src="../Distributed.png" alt="Simple CockroachDB setup"/></p><p>go-ipam was modified in a example PR here: https://github.com/metal-pod/go-ipam/pull/17</p><h3 id="API-Access"><a class="docs-heading-anchor" href="#API-Access">API Access</a><a id="API-Access-1"></a><a class="docs-heading-anchor-permalink" href="#API-Access" title="Permalink"></a></h3><p>In order to make the metal-api accessible for api users like <code>cloud-api</code> or <code>metalctl</code> as easy at it is today, some effort has to be taken. One possible approach would be to use a external loadbalancer which spread the requests evenly to all metal-api endpoints in all partitions. Because all data are accessible from all partitions, a api request going to partition A with a request to create a machine in partition B, will still work. If on the other hand partition B is not in a connected state because the interconnection between both partitions is broken, then of course the request will fail.</p><p><strong>IMPORTANT</strong> The NSQ Message to inform <code>metal-core</code> must end in the correct partition</p><p>To provide such a external loadbalancer we have several opportunities:</p><ul><li>Cloudflare or comparable CDN service.</li><li>BGP Anycast from every partition</li></ul><p>Another setup would place a small gateway behind the metal-api address, which forwards to the metal-api in the partition where the request must be executed. This gateway, <code>metal-api-router</code> must inspect the payload, extract the desired partition, and forward the request without any modifications to the metal-api endpoint in this partition. This can be done for all requests, or if we want to optimize, only for write accesses.</p><h2 id="Network-setup"><a class="docs-heading-anchor" href="#Network-setup">Network setup</a><a id="Network-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Network-setup" title="Permalink"></a></h2><p>In order to have the impact to the overall security concept as minimal as possible i would not modify the current network setup. The only modifications which has to be made are:</p><ul><li>Allow https ingress traffic to all metal-api instances.</li><li>Allow ssh ingress traffic to all metal-console instances.</li><li>Allow CockroachDB Replication between all partitions.</li><li>No NSQ traffic from outside required anymore, except we cant solve the topic above.</li></ul><p>A simple setup how this would look like is shown here, this does not work though because of the forementioned NSQ issue.</p><p><img src="../Distributed-API.png" alt="API and Console Access"/></p><p>Therefore we need the <code>metal-api-router</code>:</p><p><img src="../Distributed-API-Working.png" alt="Working API and Console Access"/></p><h2 id="Deployment"><a class="docs-heading-anchor" href="#Deployment">Deployment</a><a id="Deployment-1"></a><a class="docs-heading-anchor-permalink" href="#Deployment" title="Permalink"></a></h2><p>The deployment of our components will substantially differ in a partition compared to a the deployment we have actually. Deploying it in kubernetes in the partition would be very difficult to achieve because we have no sane way to deploy kubernetes on physical machines without a underlying API. I would therefore suggest to deploy our components in the same way we do that for the services running on the management server. Use systemd to start docker containers.</p><p><img src="../Distributed-Deployment.png" alt="Deployment"/></p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Tuesday 10 January 2023 15:35">Tuesday 10 January 2023</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
