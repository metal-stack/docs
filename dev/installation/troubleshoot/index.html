<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Troubleshoot ¬∑ metal-stack</title><meta name="title" content="Troubleshoot ¬∑ metal-stack"/><meta property="og:title" content="Troubleshoot ¬∑ metal-stack"/><meta property="twitter:title" content="Troubleshoot ¬∑ metal-stack"/><meta name="description" content="Documentation for metal-stack."/><meta property="og:description" content="Documentation for metal-stack."/><meta property="twitter:description" content="Documentation for metal-stack."/><meta property="og:url" content="https://docs.metal-stack.io/installation/troubleshoot/"/><meta property="twitter:url" content="https://docs.metal-stack.io/installation/troubleshoot/"/><link rel="canonical" href="https://docs.metal-stack.io/installation/troubleshoot/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/youtube.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="metal-stack logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">metal-stack</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><span class="tocitem">Overview</span><ul><li><a class="tocitem" href="../../overview/architecture/">Architecture</a></li><li><a class="tocitem" href="../../overview/networking/">Networking</a></li><li><a class="tocitem" href="../../overview/hardware/">Hardware Support</a></li><li><a class="tocitem" href="../../overview/os/">Operating Systems</a></li><li><a class="tocitem" href="../../overview/kubernetes/">Kubernetes Integration</a></li><li><a class="tocitem" href="../../overview/isolated-kubernetes/">Isolated Kubernetes</a></li><li><a class="tocitem" href="../../overview/gpu-support/">GPU Support</a></li><li><a class="tocitem" href="../../overview/storage/">Storage</a></li><li><a class="tocitem" href="../../overview/comparison/">Comparison</a></li></ul></li><li><a class="tocitem" href="../../quickstart/">Quickstart</a></li><li><span class="tocitem">Installation &amp; Administration</span><ul><li><a class="tocitem" href="../deployment/">Installation</a></li><li><a class="tocitem" href="../updates/">Releases and Updates</a></li><li><a class="tocitem" href="../monitoring/">Monitoring</a></li><li class="is-active"><a class="tocitem" href>Troubleshoot</a><ul class="internal"><li><a class="tocitem" href="#Deployment"><span>Deployment</span></a></li><li><a class="tocitem" href="#Operations"><span>Operations</span></a></li></ul></li></ul></li><li><span class="tocitem">User Guides</span><ul><li><a class="tocitem" href="../../external/mini-lab/README/">mini-lab</a></li><li><a class="tocitem" href="../../external/metalctl/README/">metalctl</a></li><li><a class="tocitem" href="../../external/csi-driver-lvm/README/">csi-driver-lvm</a></li><li><a class="tocitem" href="../../external/firewall-controller/README/">firewall-controller</a></li></ul></li><li><a class="tocitem" href="../../apidocs/apidocs/">API Documentation</a></li><li><span class="tocitem">Development</span><ul><li><a class="tocitem" href="../../development/client_libraries/">Client Libraries</a></li><li><a class="tocitem" href="../../development/roadmap/">Roadmap</a></li><li><a class="tocitem" href="../../development/proposals/">Enhancement Proposals</a></li><li><a class="tocitem" href="../../development/contributing/">Contributing</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Installation &amp; Administration</a></li><li class="is-active"><a href>Troubleshoot</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Troubleshoot</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/metal-stack/docs.git" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="github.com/metal-stack/docs.git" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Troubleshoot"><a class="docs-heading-anchor" href="#Troubleshoot">Troubleshoot</a><a id="Troubleshoot-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshoot" title="Permalink"></a></h1><p>This document summarizes help when something goes wrong and provides advice on debugging the metal-stack in certain situations.</p><p>Of course, it is also advisable to check out the issues on the Github projects for help.</p><p>If you still can&#39;t find a solution to your problem, please reach out to us and our community. We have a public Slack Channel to discuss problems, but you can also reach us via mail. Check out <a href="https://metal-stack.io">metal-stack.io</a> for contact information.</p><ul><li><a href="#Troubleshoot">Troubleshoot</a></li><li class="no-marker"><ul><li><a href="#Deployment">Deployment</a></li><li class="no-marker"><ul><li><a href="#Ansible-fails-when-the-metal-control-plane-helm-chart-gets-applied">Ansible fails when the metal control plane helm chart gets applied</a></li><li><a href="#In-the-mini-lab-the-control-plane-deployment-fails-because-my-system-can&#39;t-resolve-api.172.17.0.1.nip.io">In the mini-lab the control-plane deployment fails because my system can&#39;t resolve api.172.17.0.1.nip.io</a></li><li class="no-marker"><ul><li><a href="#FritzBox">FritzBox</a></li></ul></li></ul></li><li><a href="#Operations">Operations</a></li><li class="no-marker"><ul><li><a href="#Fixing-Machine-Issues">Fixing Machine Issues</a></li><li class="no-marker"><ul><li><a href="#no-event-container">no-event-container</a></li><li><a href="#no-partition">no-partition</a></li><li><a href="#liveliness-dead">liveliness-dead</a></li><li><a href="#liveliness-unknown">liveliness-unknown</a></li><li><a href="#liveliness-not-available">liveliness-not-available</a></li><li><a href="#failed-machine-reclaim">failed-machine-reclaim</a></li><li><a href="#crashloop">crashloop</a></li><li><a href="#last-event-error">last-event-error</a></li><li><a href="#asn-not-unique">asn-not-unique</a></li><li><a href="#bmc-without-mac">bmc-without-mac</a></li><li><a href="#bmc-without-ip">bmc-without-ip</a></li><li><a href="#bmc-no-distinct-ip">bmc-no-distinct-ip</a></li><li><a href="#bmc-info-outdated">bmc-info-outdated</a></li></ul></li><li><a href="#A-machine-has-registered-with-a-different-UUID-after-reboot">A machine has registered with a different UUID after reboot</a></li><li class="no-marker"><ul><li><a href="#Reasons">Reasons</a></li><li><a href="#Solution">Solution</a></li></ul></li><li><a href="#Fixing-Switch-Issues">Fixing Switch Issues</a></li><li class="no-marker"><ul><li><a href="#switch-sync-failing">switch-sync-failing</a></li></ul></li><li><a href="#Switch-Replacement-and-Migration">Switch Replacement and Migration</a></li><li class="no-marker"><ul><li><a href="#Replacing-a-Switch">Replacing a Switch</a></li><li><a href="#Migrating-from-one-Switch-to-another">Migrating from one Switch to another</a></li><li><a href="#Preconditions-for-Migration-and-Replacement">Preconditions for Migration and Replacement</a></li><li><a href="#Migrating-from-Cumulus-to-Edgecore-SONiC">Migrating from Cumulus to Edgecore SONiC</a></li></ul></li></ul></li></ul></li></ul><h2 id="Deployment"><a class="docs-heading-anchor" href="#Deployment">Deployment</a><a id="Deployment-1"></a><a class="docs-heading-anchor-permalink" href="#Deployment" title="Permalink"></a></h2><h3 id="Ansible-fails-when-the-metal-control-plane-helm-chart-gets-applied"><a class="docs-heading-anchor" href="#Ansible-fails-when-the-metal-control-plane-helm-chart-gets-applied">Ansible fails when the metal control plane helm chart gets applied</a><a id="Ansible-fails-when-the-metal-control-plane-helm-chart-gets-applied-1"></a><a class="docs-heading-anchor-permalink" href="#Ansible-fails-when-the-metal-control-plane-helm-chart-gets-applied" title="Permalink"></a></h3><p>There can be many reasons for this. Since you are deploying the metal control plane into a Kubernetes cluster, the first step should be to install <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a> and check the pods in your cluster. Depending on the metal-stack version and Kubernetes cluster, your control-plane should look something like this after the deployment (this is in a Kind cluster):</p><pre><code class="language-bash hljs">kubectl get pod -A
NAMESPACE             NAME                                         READY   STATUS      RESTARTS   AGE
ingress-nginx         nginx-ingress-controller-56966f7dc7-khfp9    1/1     Running     0          2m34s
kube-system           coredns-66bff467f8-grn7q                     1/1     Running     0          2m34s
kube-system           coredns-66bff467f8-n7n77                     1/1     Running     0          2m34s
kube-system           etcd-kind-control-plane                      1/1     Running     0          2m42s
kube-system           kindnet-4dv7m                                1/1     Running     0          2m34s
kube-system           kube-apiserver-kind-control-plane            1/1     Running     0          2m42s
kube-system           kube-controller-manager-kind-control-plane   1/1     Running     0          2m42s
kube-system           kube-proxy-jz7kp                             1/1     Running     0          2m34s
kube-system           kube-scheduler-kind-control-plane            1/1     Running     0          2m42s
local-path-storage    local-path-provisioner-bd4bb6b75-cwfb7       1/1     Running     0          2m34s
metal-control-plane   ipam-db-0                                    2/2     Running     0          2m31s
metal-control-plane   masterdata-api-6dd4b54db5-rwk45              1/1     Running     0          33s
metal-control-plane   masterdata-db-0                              2/2     Running     0          2m29s
metal-control-plane   metal-api-998cb46c4-jj2tt                    1/1     Running     0          33s
metal-control-plane   metal-api-initdb-r9sc6                       0/1     Completed   0          2m24s
metal-control-plane   metal-api-liveliness-1590479940-brhc7        0/1     Completed   0          6s
metal-control-plane   metal-console-7955cbb7d7-p6hxp               1/1     Running     0          33s
metal-control-plane   metal-db-0                                   2/2     Running     0          2m34s
metal-control-plane   nsq-lookupd-5b4ccbfb64-n6prg                 1/1     Running     0          2m34s
metal-control-plane   nsqd-6cd87f69c4-vtn9k                        2/2     Running     0          2m33s</code></pre><p>If there are any failing pods, investigate those and look into container logs. This information should point you to the place where the deployment goes wrong.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Sometimes, you see a helm errors like &quot;no deployed releases&quot; or something like this. When a helm chart fails after the first deployment it could be that you have a chart installation still pending. Also, the control plane helm chart uses pre- and post-hooks, which creates <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">jobs</a> that helm expects to be completed before attempting another deployment. Delete the helm chart (use Helm 3) with <code>helm delete -n metal-control-plane metal-control-plane</code> and delete the jobs in the <code>metal-control-plane</code> namespace before retrying the deployment.</p></div></div><h3 id="In-the-mini-lab-the-control-plane-deployment-fails-because-my-system-can&#39;t-resolve-api.172.17.0.1.nip.io"><a class="docs-heading-anchor" href="#In-the-mini-lab-the-control-plane-deployment-fails-because-my-system-can&#39;t-resolve-api.172.17.0.1.nip.io">In the mini-lab the control-plane deployment fails because my system can&#39;t resolve api.172.17.0.1.nip.io</a><a id="In-the-mini-lab-the-control-plane-deployment-fails-because-my-system-can&#39;t-resolve-api.172.17.0.1.nip.io-1"></a><a class="docs-heading-anchor-permalink" href="#In-the-mini-lab-the-control-plane-deployment-fails-because-my-system-can&#39;t-resolve-api.172.17.0.1.nip.io" title="Permalink"></a></h3><p>The control-plane deployment returns an error like this:</p><pre><code class="language-bash hljs">deploy-control-plane | fatal: [localhost]: FAILED! =&gt; changed=false
deploy-control-plane |   attempts: 60
deploy-control-plane |   content: &#39;&#39;
deploy-control-plane |   elapsed: 0
deploy-control-plane |   msg: &#39;Status code was -1 and not [200]: Request failed: &lt;urlopen error [Errno -5] No address associated with hostname&gt;&#39;
deploy-control-plane |   redirected: false
deploy-control-plane |   status: -1
deploy-control-plane |   url: http://api.172.17.0.1.nip.io:8080/metal/v1/health
deploy-control-plane |
deploy-control-plane | PLAY RECAP *********************************************************************
deploy-control-plane | localhost                  : ok=29   changed=4    unreachable=0    failed=1    skipped=7    rescued=0    ignored=0
deploy-control-plane |
deploy-control-plane exited with code 2</code></pre><p>Some home routers have a security feature that prevents DNS Servers to resolve anything in the router&#39;s local IP range (DNS-Rebind-Protection).</p><p>You need to add an exception for <code>nip.io</code> in your router configuration or add <code>127.0.0.1    api.172.17.0.1.nip.io</code> to your <code>/etc/hosts</code>.</p><h4 id="FritzBox"><a class="docs-heading-anchor" href="#FritzBox">FritzBox</a><a id="FritzBox-1"></a><a class="docs-heading-anchor-permalink" href="#FritzBox" title="Permalink"></a></h4><p><code>Home Network -&gt; Network -&gt; Network Settings -&gt; Additional Settings -&gt; DNS Rebind Protection -&gt; Host name exceptions -&gt; nip.io</code></p><h2 id="Operations"><a class="docs-heading-anchor" href="#Operations">Operations</a><a id="Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Operations" title="Permalink"></a></h2><h3 id="Fixing-Machine-Issues"><a class="docs-heading-anchor" href="#Fixing-Machine-Issues">Fixing Machine Issues</a><a id="Fixing-Machine-Issues-1"></a><a class="docs-heading-anchor-permalink" href="#Fixing-Machine-Issues" title="Permalink"></a></h3><p>The <code>metalctl machine issues</code> command gives you an overview over machines in your metal-stack environment that are in an unusual state.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>Machines that are known not to function properly, should be locked through <code>metalctl machine lock</code> and annotated with a description of the problem. This way, you can mark machine for replacement without being in danger of having a user allocating the faulty machine.</p></div></div><p>In the following sections, you can look up the machine issues that are returned by <code>metalctl</code> and find out how to deal with them properly.</p><h4 id="no-event-container"><a class="docs-heading-anchor" href="#no-event-container">no-event-container</a><a id="no-event-container-1"></a><a class="docs-heading-anchor-permalink" href="#no-event-container" title="Permalink"></a></h4><p>Every machine in the metal-stack database usually has a corresponding event container where provisioning events are stored. This database entity gets created lazily as soon as a machine is registered by the metal-hammer or a provisioning event for the machine arrives at the metal-api.</p><p>When there is no event container, this means that the machine has never registered nor received a provisioning event. As an operator you should evaluate why this machine is not booting into the metal-hammer.</p><p>This issue is special in a way that it prevents other issues from being evaluated for this machine because the issue calculation usually requires information from the machine event container.</p><h4 id="no-partition"><a class="docs-heading-anchor" href="#no-partition">no-partition</a><a id="no-partition-1"></a><a class="docs-heading-anchor-permalink" href="#no-partition" title="Permalink"></a></h4><p>When a machine has no partition, the <a href="https://github.com/metal-stack/metal-hammer">metal-hammer</a> has not yet registered the machine at the <a href="https://github.com/metal-stack/metal-api">metal-api</a>. Instead, the machine was created through metal-stack&#39;s event machinery, which does not have a lot of information about a machine (e.g. a PXE boot event was reported from the pixiecore), or just by the <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> which discovered the machine through DHCP.</p><p>This can usually happen on the very first boot of a machine and the machine&#39;s <a href="../../overview/hardware/">hardware is not supported</a> by metal-stack, leading to the <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> being unable to report BMC details to the metal-api (a metal-bmc report sets the partition id of a machine) and the metal-hammer not finishing the machine registration phase.</p><p>To resolve this issue, you need to identify the machine in your metal-stack partition that emits PXE boot events and find the reason why it is not properly booting into the metal-hammer. The console logs of this machine should enable you to find out the root cause.</p><h4 id="liveliness-dead"><a class="docs-heading-anchor" href="#liveliness-dead">liveliness-dead</a><a id="liveliness-dead-1"></a><a class="docs-heading-anchor-permalink" href="#liveliness-dead" title="Permalink"></a></h4><p>For machines without an allocation, the metal-hammer consistently reports whether a machine is still being responsive or not. When the liveliness is <code>Dead</code>, there were no events received from this machine for longer than ~5 minutes.</p><p>Reasons for this can be:</p><ul><li>The network connection between the partition and metal-stack control plane is interrupted</li><li>The machine was removed from your data center</li><li>The machine has changed its UUID <a href="https://github.com/metal-stack/metal-hammer/issues/52">metal-hammer#52</a></li><li>The machine is turned off</li><li>The machine hangs / freezes</li><li>The machine booted to BIOS or UEFI shell and does not try to PXE boot again</li><li>The issue only appears temporarily<ul><li>The machine takes longer than 5 minutes for the reboot</li><li>The machine is performing a firmware upgrade, which usually takes longer than 5 minutes to succeed</li></ul></li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>In order to minimize maintenance overhead, a machine which is dead for longer than an hour will be rebooted through the metal-api.</p><p>In case you want to prevent this action from happening for a machine, you can lock the machine through <code>metalctl machine lock</code>.</p></div></div><p>If the machine is dead for a long time and you are sure that it will never come back, you can clean up the machine through <code>metalctl machine rm --remove-from-database</code>.</p><h4 id="liveliness-unknown"><a class="docs-heading-anchor" href="#liveliness-unknown">liveliness-unknown</a><a id="liveliness-unknown-1"></a><a class="docs-heading-anchor-permalink" href="#liveliness-unknown" title="Permalink"></a></h4><p>For machines that are allocated by a user, the ownership has gone over to this user and as an operator you cannot access the machine anymore. This makes it harder to detect whether a machine is in a healthy state or not. Typically, all official metal-stack OS images deploy an LLDP daemon, that consistently emits alive messages. These messages are caught by the <a href="https://github.com/metal-stack/metal-core">metal-core</a> and turned into a <code>Phoned Home</code> event. Internally, the metal-api uses these events as an indicator to decide whether the machine is still responsive or not.</p><p>When the LLDP daemon stopped sending packages, the reasons are identical to those of <a href="#liveliness-dead">dead machines</a>. However, it&#39;s not possible anymore to decide whether the user is responsible for reaching this state or not.</p><p>In most of the cases, there is not much that can be done from the operator&#39;s perspective. You will need to wait for the user to report an issue with the machine. When you do support, you can use this issue type to quickly identify this machine.</p><h4 id="liveliness-not-available"><a class="docs-heading-anchor" href="#liveliness-not-available">liveliness-not-available</a><a id="liveliness-not-available-1"></a><a class="docs-heading-anchor-permalink" href="#liveliness-not-available" title="Permalink"></a></h4><p>This is more of a theoretical issue. When the machine liveliness is not available check that the Kubernetes <code>CronJob</code> in the metal-stack control plane for evaluating the machine liveliness is running regularly and not containing error logs. Make the machine boot into the metal-hammer and this issue should not appear.</p><h4 id="failed-machine-reclaim"><a class="docs-heading-anchor" href="#failed-machine-reclaim">failed-machine-reclaim</a><a id="failed-machine-reclaim-1"></a><a class="docs-heading-anchor-permalink" href="#failed-machine-reclaim" title="Permalink"></a></h4><p>If a machine remains in the <code>Phoned Home</code> state without having an allocation, this indicates that the <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> was not able to put the machine back into PXE boot mode after <code>metalctl machine rm</code>. The machine is still running the operating system and it does not return back into the allocatable machine pool. Effectively, you lost a machine in your environment and no-one pays for it. Therefore, you should resolve this issue as soon as possible.</p><p>In bad scenarios, when the machine was a firewall, the machine can still reach the internet through the PXE boot network and also attract traffic, which it cannot route anymore inside the tenant VRF. This can cause traffic loss inside a tenant network.</p><p>In most of the cases, it should be sufficient to run another <code>metalctl machine rm</code> on this machine in order to retry booting into PXE mode. If this still does not succeed, you can boot the machine into the BIOS and manually and change the boot order to PXE boot. This should force booting the metal-hammer again and add the machine back into your pool of allocatable machines.</p><p>For further reference, see <a href="https://github.com/metal-stack/metal-api/issues/145">metal-api#145</a>.</p><h4 id="crashloop"><a class="docs-heading-anchor" href="#crashloop">crashloop</a><a id="crashloop-1"></a><a class="docs-heading-anchor-permalink" href="#crashloop" title="Permalink"></a></h4><p>Under bad circumstances, a machine diverges from its typical machine lifecycle. When this happens, the internal state-machine of the metal-api detects that the machine reboots unexpectedly during the provisioning phase. It is likely that the machine has entered a crash loop where it PXE boots again and again without the machine ever becoming usable.</p><p>Reasons for this can be:</p><ul><li>The machine&#39;s <a href="../../overview/hardware/">hardware is not supported</a> and the metal-hammer crashes during the machine discovery</li><li>The machine registration fails through the metal-hammer because an orphaned / dead machine is still present in the metal-api&#39;s data base. The machine is connected to the same switch ports that were used by the orphaned machine. In this case, you should clean up the orphaned machine through <code>metalctl machine rm --remove-from-database</code>.</li></ul><p>Please also consider console logs of the machine for investigating the issue.</p><p>The incomplete cycle count is reset as soon as the machine reaches <code>Phoned Home</code> state or there is a <code>Planned Reboot</code> of the machine (planned reboot is also done by the metal-hammer once a day in order to reboot with the latest version).</p><h4 id="last-event-error"><a class="docs-heading-anchor" href="#last-event-error">last-event-error</a><a id="last-event-error-1"></a><a class="docs-heading-anchor-permalink" href="#last-event-error" title="Permalink"></a></h4><p>The machine had an error during the provisioning lifecycle recently or events are arriving out of order at the metal-api. This can be an interesting hint for the operator that something during machine provisioning went wrong. You can look at the error through <code>metalctl machine describe</code> or <code>metalctl machine logs</code>.</p><p>This error will disappear after a certain time period from <code>machine issues</code>. You can still look up the error as described above.</p><h4 id="asn-not-unique"><a class="docs-heading-anchor" href="#asn-not-unique">asn-not-unique</a><a id="asn-not-unique-1"></a><a class="docs-heading-anchor-permalink" href="#asn-not-unique" title="Permalink"></a></h4><p>This issue was introduced by a bug in earlier versions of metal-stack and was fixed in <a href="https://github.com/metal-stack/metal-api/pull/105.">PR105</a></p><p>To resolve the issue, you need to recreate the firewalls that use the same ASN.</p><h4 id="bmc-without-mac"><a class="docs-heading-anchor" href="#bmc-without-mac">bmc-without-mac</a><a id="bmc-without-mac-1"></a><a class="docs-heading-anchor-permalink" href="#bmc-without-mac" title="Permalink"></a></h4><p>The <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> is responsible to report connection data for the machine&#39;s <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller">BMC</a>.</p><p>If it&#39;s uncapable of discovering this information, your <a href="../../overview/hardware/">hardware might not be supported</a>. Please investigate the logs of the metal-bmc to find out what&#39;s going wrong with this machine.</p><h4 id="bmc-without-ip"><a class="docs-heading-anchor" href="#bmc-without-ip">bmc-without-ip</a><a id="bmc-without-ip-1"></a><a class="docs-heading-anchor-permalink" href="#bmc-without-ip" title="Permalink"></a></h4><p>The <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> is responsible to report connection data for the machine&#39;s <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller">BMC</a>.</p><p>If it&#39;s uncapable of discovering this information, your <a href="../../overview/hardware/">hardware might not be supported</a>. Please investigate the logs of the metal-bmc to find out what&#39;s going wrong with this machine.</p><h4 id="bmc-no-distinct-ip"><a class="docs-heading-anchor" href="#bmc-no-distinct-ip">bmc-no-distinct-ip</a><a id="bmc-no-distinct-ip-1"></a><a class="docs-heading-anchor-permalink" href="#bmc-no-distinct-ip" title="Permalink"></a></h4><p>The <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> is responsible to report connection data for the machine&#39;s <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller">BMC</a>.</p><p>When there is no distinct IP address for the BMC, it can be that an orphaned machine used this IP in the past. In this case, you need to clean up the orphaned machine through <code>metalctl machine rm --remove-from-database</code>.</p><h4 id="bmc-info-outdated"><a class="docs-heading-anchor" href="#bmc-info-outdated">bmc-info-outdated</a><a id="bmc-info-outdated-1"></a><a class="docs-heading-anchor-permalink" href="#bmc-info-outdated" title="Permalink"></a></h4><p>The <a href="https://github.com/metal-stack/metal-bmc">metal-bmc</a> is responsible to report bmc details for the machine&#39;s <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface#Baseboard_management_controller">BMC</a>.</p><p>When the metal-bmc was not able to fetch the bmc info for longer than 20 minutes, something is wrong with the BMC configuration of the machine. This can be caused by one of the following reasons:</p><ul><li>Wrong password for the root user is configured in the BMC</li><li>ip address of the BMC is either wrong or not present</li><li>the device on the given ip address is not a machine, maybe a switch or a management component which is not managed by the metal-api</li></ul><p>In either case, please check the logs for the given machine UUID on the metal-bmc for further details. Also check that the metal-bmc is configured to only consider BMC IPs in the range they are configured from the DHCP server in the partition. This prevents grabbing unrelated BMCs.</p><h3 id="A-machine-has-registered-with-a-different-UUID-after-reboot"><a class="docs-heading-anchor" href="#A-machine-has-registered-with-a-different-UUID-after-reboot">A machine has registered with a different UUID after reboot</a><a id="A-machine-has-registered-with-a-different-UUID-after-reboot-1"></a><a class="docs-heading-anchor-permalink" href="#A-machine-has-registered-with-a-different-UUID-after-reboot" title="Permalink"></a></h3><p>metal-stack heavily relies on steady machine UUIDs as the UUID is the primary key of the machine entity in the metal-api.</p><p>For further reference also see <a href="https://github.com/metal-stack/metal-hammer/issues/52">metal-stack/metal-hammer#52</a>.</p><h4 id="Reasons"><a class="docs-heading-anchor" href="#Reasons">Reasons</a><a id="Reasons-1"></a><a class="docs-heading-anchor-permalink" href="#Reasons" title="Permalink"></a></h4><p>There are some scenarios (can be vendor-specific), which can cause a machine UUID to change over time, e.g.:</p><ul><li>When the UUID partly contains of a network card&#39;s mac address, it can happen when:<ul><li>Exchanging network cards</li><li>Disabling network cards through BIOS</li></ul></li><li>Changing the UUID through vendor-specific CLI tool</li></ul><h4 id="Solution"><a class="docs-heading-anchor" href="#Solution">Solution</a><a id="Solution-1"></a><a class="docs-heading-anchor-permalink" href="#Solution" title="Permalink"></a></h4><ol><li>After five minutes, the orphaned machine UUID will be marked dead (üíÄ) because machine events will be sent only to the most recent UUID</li><li>Identify the dead machine through <code>metalctl machine ls</code></li><li>Remove the dead machine forcefully with <code>metalctl machine rm --remove-from-database --yes-i-really-mean-it &lt;uuid&gt;</code></li></ol><h3 id="Fixing-Switch-Issues"><a class="docs-heading-anchor" href="#Fixing-Switch-Issues">Fixing Switch Issues</a><a id="Fixing-Switch-Issues-1"></a><a class="docs-heading-anchor-permalink" href="#Fixing-Switch-Issues" title="Permalink"></a></h3><h4 id="switch-sync-failing"><a class="docs-heading-anchor" href="#switch-sync-failing">switch-sync-failing</a><a id="switch-sync-failing-1"></a><a class="docs-heading-anchor-permalink" href="#switch-sync-failing" title="Permalink"></a></h4><p>For your network infrastructure it is key to adapt to new configuration. In case this sync process fails for more than 10 minutes, it is likely to require manual investigation.</p><p>Depending on your switch operating system, the error sources might differ a lot. Try to connect to your switch using the console or ssh and investigate the logs. Check if the hard drive is full.</p><h3 id="Switch-Replacement-and-Migration"><a class="docs-heading-anchor" href="#Switch-Replacement-and-Migration">Switch Replacement and Migration</a><a id="Switch-Replacement-and-Migration-1"></a><a class="docs-heading-anchor-permalink" href="#Switch-Replacement-and-Migration" title="Permalink"></a></h3><p>There are two mechanisms to replace an existing switch with a new one, both of which will transfer existing VRF configuration and machine connections from one switch to another. Due to the redundance of the CLOS topology, a switch replacement can be performed without downtime.</p><h4 id="Replacing-a-Switch"><a class="docs-heading-anchor" href="#Replacing-a-Switch">Replacing a Switch</a><a id="Replacing-a-Switch-1"></a><a class="docs-heading-anchor-permalink" href="#Replacing-a-Switch" title="Permalink"></a></h4><p>If the new switch should have the same ID as the old one you should perform a switch replacement. To find detailed information about the procedure of a switch replacement use <code>metalctl switch replace --help</code>. Basically, what you need to do is mark the switch for replacement via <code>metalctl switch replace</code>, then physically replace the switch with the new one and configure it. The last step is to deploy metal-core on the switch. Once metal-core registers the new switch at the metal-api, the old switches configuration and machine connections will be transferred to the new one. Note that the replacement only works if the new switch has the same ID as the old one. Otherwise metal-core will simply register a new switch and leave the old one untouched.</p><h4 id="Migrating-from-one-Switch-to-another"><a class="docs-heading-anchor" href="#Migrating-from-one-Switch-to-another">Migrating from one Switch to another</a><a id="Migrating-from-one-Switch-to-another-1"></a><a class="docs-heading-anchor-permalink" href="#Migrating-from-one-Switch-to-another" title="Permalink"></a></h4><p>If the new switch should not or cannot have the same ID as the old one, then the <code>switch migrate</code> command can be used to achieve the same result as a switch replacement. Perform the following steps:</p><ol><li>Leave the old switch in place.</li><li>Install the new switch in the rack without connecting it to any machines yet.</li><li>Adjust the metal-stack deployment in the same way as for a switch replacement.</li><li>Deploy metal-core on the new switch and wait for it to register at the metal-api. Once the switch is registered it will be listed when you run <code>metalctl switch ls</code>.</li><li>Run <code>metalctl switch migrate &lt;old-switch-id&gt; &lt;new-switch-id&gt;</code>.</li><li>Disconnect all machines from the old switch and connect them to the new one.</li></ol><p>In between steps 5 and 6 there is a mismatch between the switch-machine-connections known to the metal-api and the real connections. Since the metal-api learns about the connections from what a machine reports during registration, a machine registration that occurs in between steps 5 and 6 will result in a condition that looks somewhat broken. The metal-api will think that a machine is connected to three switches. This, however, should not cause any problems. Just move on to step 6 and delete the old switch from the metal-api afterwards. If the case just described really occurs, then <code>metalctl switch delete &lt;old-switch-id&gt;</code> will throw an error, because deleting a switch with existing machine connections might be dangerous. If, apart from that, the migration was successful, then the old switch can be safely deleted with <code>metalctl switch delete &lt;old-switch-id&gt; --force</code>.</p><h4 id="Preconditions-for-Migration-and-Replacement"><a class="docs-heading-anchor" href="#Preconditions-for-Migration-and-Replacement">Preconditions for Migration and Replacement</a><a id="Preconditions-for-Migration-and-Replacement-1"></a><a class="docs-heading-anchor-permalink" href="#Preconditions-for-Migration-and-Replacement" title="Permalink"></a></h4><p>An invariant that must be satisfied throughout is that the switch ports a machine is connected to must match, i.e. a machine connected to <code>Ethernet0</code> on switch 1 must be connected to <code>Ethernet0</code> on switch 2 etc. Furthermore, the breakout configurations of both switches must match and the new switch must contain at least all of the old switch&#39;s interfaces.</p><h4 id="Migrating-from-Cumulus-to-Edgecore-SONiC"><a class="docs-heading-anchor" href="#Migrating-from-Cumulus-to-Edgecore-SONiC">Migrating from Cumulus to Edgecore SONiC</a><a id="Migrating-from-Cumulus-to-Edgecore-SONiC-1"></a><a class="docs-heading-anchor-permalink" href="#Migrating-from-Cumulus-to-Edgecore-SONiC" title="Permalink"></a></h4><p>Both migration and replacement can be used to move from Cumulus to Edgecore SONiC (or vice versa). Migrating to or from Broadcom SONiC or mixing Broadcom SONiC with Cumulus or Edgecore SONiC is not supported.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../monitoring/">¬´ Monitoring</a><a class="docs-footer-nextpage" href="../../external/mini-lab/README/">mini-lab ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Wednesday 9 April 2025 07:15">Wednesday 9 April 2025</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
