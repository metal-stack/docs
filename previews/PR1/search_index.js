var documenterSearchIndex = {"docs":
[{"location":"proposals/MEP2/README/#Two-Factor-Authentication-1","page":"Two Factor Authentication","title":"Two Factor Authentication","text":"","category":"section"},{"location":"proposals/MEP4/README/#Multi-tenancy-for-the-metal-api-1","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"In the past we decided to treat the metal-api as a \"low-level API\", i.e. the API does not know anything about projects and tenants. A user with editor access can for example assign machines to every project he desires, he can see all the machines available and control them. Even though we always wanted to keep open the possibility to just offer bare metal machines to the end-user, the ultimate objective has always been to create an API for Kubernetes clusters. Hence, we tried to keep the metal-api code base as small as possible and we added resource scoping to a \"higher-level API\", the cloud-api. From there, a user would only be able to see his own clusters and IP addresses. The cloud-api is a component that is not open-source.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"The implication is that the metal-api has no multi-tenancy without another layer on top of it that implements resource scoping. We treat clusters as first-class citizens and fulfill the objective that we had from the very beginning: give clusters to the end-users.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"However, as time passed by, things changed: The Metal Stack is becoming an open-source product and we already have promising adopters of our product, who are willing to contribute to Metal Stack. This is a serious chance of making our product better and more successful. It turns out that the decision we made is sufficient for us, but for others it is not.","category":"page"},{"location":"proposals/MEP4/README/#Why-adopters-need-multi-tenancy-in-the-metal-api-1","page":"Multi-tenancy for the metal-api","title":"Why adopters need multi-tenancy in the metal-api","text":"","category":"section"},{"location":"proposals/MEP4/README/#Not-every-adopter-will-be-interested-in-the-cloud-api-1","page":"Multi-tenancy for the metal-api","title":"Not every adopter will be interested in the cloud-api","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"For example, users who want to combine the Metal Stack with Gardener, may not want to hide all of the Gardener's functionality behind the cloud-api in the way we do. They want to use the much more powerful Gardener Dashboard instead. The Gardener itself does not need the cloud-api either. It is a cluster-api by itself. It only needs to utilize our \"low-level API\" and actually expects this API to have multi-tenancy as otherwise every logged in user can create / destroy clusters in every existing project from the Gardener dashboard.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"This makes obvious that, with our decision, we placed an unnecessary obstacle in the way of our adopters: They now need to implement an own layer between the Gardener and the metal-api to provide multi-tenancy. From the Gardener-perspective we strongly differ from other cloud providers in this aspect and it is a matter of time when this will become an issue. When we encourage adopters to implement such interfaces on their own we also partly lose control of our product, we increase divergence.","category":"page"},{"location":"proposals/MEP4/README/#We-cannot-claim-that-Metal-Stack-is-a-multi-tenant-solution-on-our-website-1","page":"Multi-tenancy for the metal-api","title":"We cannot claim that Metal Stack is a multi-tenant solution on our website","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"As the cloud-api is not part of the Metal Stack, the promise of multi-tenancy is only true for our network layer. Without the cloud-api to enable multi-tenancy, the network isolation is currently useless for end-users. Users of the Metal Stack can not self-manage machines, networks and ips without compromising the environment and thus, there is no self-service. We lose a valuable selling point when adopters can not immediately make use of our leading edge network isolation where we put so much effort to.","category":"page"},{"location":"proposals/MEP4/README/#Open-partitions-for-third-party-usage-1","page":"Multi-tenancy for the metal-api","title":"Open partitions for third-party usage","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"If a third-party uses Gardener and our metal-api had multi-tenancy, we would be able to allow a third-party to create clusters with workers in our own partitions. At the moment, this is not possible because the Gardener needs to know the HMAC secrets to create worker nodes, which would compromise our environment. If a thirdy-party knows our HMAC we lose control over the machines of our own tenants.","category":"page"},{"location":"proposals/MEP4/README/#We-do-not-actually-want-to-open-source-the-cloud-api-1","page":"Multi-tenancy for the metal-api","title":"We do not actually want to open-source the cloud-api","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"One could think about solving the multi-tenancy issue by adding machine endpoints to the cloud-api. Gardener would then not consume the metal-api anymore but only the cloud-api.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"This approach would not be ideal. We only want to offer a minimum viable product to adopters. The Gardener does not need a cluster-api as provided by the cloud-api. We want to treat additions on top of the basic stack as enterprise products.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"The cloud-api contains billing endpoints, which are a perfect example for an optional addition of the Metal Stack. For basic usage of the Metal Stack a user does not need billing. Still, billing functionality can be interesting for some enterprises, who are like us, selling the infrastructure to third-parties.","category":"page"},{"location":"proposals/MEP4/README/#Increased-security-for-provider-admins-1","page":"Multi-tenancy for the metal-api","title":"Increased security for provider admins","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"Multi-tenancy in the metal-api also has the potential to limit the damage that a provider administrator can cause by mistake. If an administrator has to acquire project permissions on machine-level we can effectively reduce the damage he can make to this single project.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"Another example would be the automatic provisioning of a Gitlab CI runner used for integration testing (a use case that we have where we do not require the cloud-api). This can easily be done in automated manner with Ansible and the Metal dynamic inventory + modules. However, with Ansible, mistakes in the automation can be made very quickly and if Ansible would only see machines of a dedicated project, this would also reduce damage it can make.","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"It is likely that there are more similar use-cases like that to come (maybe even for the storage solution?).","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"Also the surface for our Gardener components (metal-ccm, gardener-extension-provider-metal, machine-controller-manager) would be reduced to project scopes.","category":"page"},{"location":"proposals/MEP4/README/#Conclusion-1","page":"Multi-tenancy for the metal-api","title":"Conclusion","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"For these reasons the decision we made is very likely to have a negative impact on the adoption-rate of the Metal Stack and we should think about treating machines, networks and ips as first-class citizens as well. This makes us closer to the offer of hyperscalers. As mentioned in the beginning, all the time we tried to keep the possibility open to just offer bare metal machines. Let's continue with decision by adding multi-tenancy to the metal-api.","category":"page"},{"location":"proposals/MEP4/README/#Required-actions-1","page":"Multi-tenancy for the metal-api","title":"Required actions","text":"","category":"section"},{"location":"proposals/MEP4/README/#Resource-scoping-1","page":"Multi-tenancy for the metal-api","title":"Resource scoping","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"Just as implemented by the cloud-api, resource scoping needs to be added to almost every endpoint of the metal-api:","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"Machines / Firewalls\nA user should only be able to view machines / firewalls of the projects he has at least view access to\nA user should only be able to create and destroy machines / firewalls for projects he has at least editor access to Provider-tenants with at least view access can additionally view machines which have no project assignments Provider-tenants with at least editor access can additionally allocate / reserve machines which have no project assignments\nNetworks\nA user should only be able to view networks of the projects he has at least view access to\nA user should only be able to allocate networks of projects he has at least editor access to\nA user should only be able to free networks assigned to projects he has at least editor access to Provider-tenants with at least view access can additionally view networks which have no project assignments Provider-tenants with at least editor access can additionally edit networks which have no project assignments Provider-tenants with at least admin access can additionally create or remove networks which have no project assignments\nIPs\nA user should only be able to view ips of the projects he has at least view access to\nA user should only be able to allocate ips in networks of projects he has at least editor access to\nA user should only be able to free ips assigned to projects he has at least editor access to\nProjects\nA logged in user is able to create projects when he has the permission to create projects\nA user should only be able to view projects where he has at least view access to\nA user should only be able to delete projects where he has admin access to\nPartitions / Images\nOnly provider-admin users can add, delete, update\nAll logged in users can view\nIPMI\nOnly provider-tenants can view machine IPMI data\nEndpoints for internal use\nShould only be accessible with HMAC auth and the HMAC secrets are only known by components of the Metal Stack (mainly for communication between partition and control plane), never for third-party usage","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"For all of this we need enhance the database queries with a filter for projects that a user has access to. As we already use a client to the masterdata-api in the metal-api, we can extract project memberships of a logged in user from there.","category":"page"},{"location":"proposals/MEP4/README/#More-permissions-1","page":"Multi-tenancy for the metal-api","title":"More permissions","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"We do not only need kaas-... permissions in the LDAP but also maas-. This way we can differentiate between permissions for the cloud-api and permissions for the metal-api.","category":"page"},{"location":"proposals/MEP4/README/#Service-account-tokens-/-technical-users-1","page":"Multi-tenancy for the metal-api","title":"Service account tokens / technical users","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"We need to provide the possibility for users to obtain access tokens to use for technical purposes (CI, third-party tooling like Gardener, ...).","category":"page"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"We do not have this functionality yet, but it would also become a necessity for the cloud-api at some point in the future.","category":"page"},{"location":"proposals/MEP4/README/#Cloud-API-1","page":"Multi-tenancy for the metal-api","title":"Cloud API","text":"","category":"section"},{"location":"proposals/MEP4/README/#","page":"Multi-tenancy for the metal-api","title":"Multi-tenancy for the metal-api","text":"Project creation and deletion again have to be moved back into the metal-api, this also frees adopters from the need to write an own API in order to manage projects- The cloud-api will (again) only proxy project endpoints through to the metal-api\nDo not point the secret bindings to a the shared provider secret in a partition. Create an individual provider-secret for the logged in tenant. The Gardener needs to use this tenant-specific provider secret to talk to the metal-api, do not give the Gardener HMAC access anymore.\nThe provider secret partition mapping can be removed from the cloud-api config and from the deployment","category":"page"},{"location":"guides/#Getting-Started-1","page":"Guides","title":"Getting Started","text":"","category":"section"},{"location":"guides/#","page":"Guides","title":"Guides","text":"Before starting to buy any expensive hardware, you should try out the metal-stack on your notebook and familiarize with all the major principles.","category":"page"},{"location":"guides/#","page":"Guides","title":"Guides","text":"For this, we made the mini-lab.","category":"page"},{"location":"guides/#","page":"Guides","title":"Guides","text":"The mini-lab is a fully virtual setup and is supposed to be run on a single notebook. For this reason, the setup is slightly simplified in comparison to full-blown setups on real hardware. However, the lab should help to understand all major ideas and components of the metal-stack.","category":"page"},{"location":"contributing/#Contributing-1","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"This document describes the way we want to contribute code to the projects of metal-stack, which are hosted on github.com/metal-stack.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"The document is meant to be understood as a general guideline for contributions, but not as burden to be placed on a developer. Use your best judgment when contributing code. Try to be as clean and precise as possible when writing code and try to make your code as maintainable and understandable as possible for other people.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Even if it should go without saying, we live an open culture of discussion, in which everybody is welcome to participate. We treat every contribution with respect and objectiveness with the general aim to write software of quality.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"If you want, feel free to propose changes to this document in a pull request.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Pages = [\"contributing.md\"]\nDepth = 5","category":"page"},{"location":"contributing/#How-Can-I-Contribute?-1","page":"Contributing","title":"How Can I Contribute?","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Open a Github issue in the project you would like to contribute. Within the issue, your idea can be discussed. It is also possible to directly create a pull request when the set of changes is relatively small.","category":"page"},{"location":"contributing/#Pull-Requests-1","page":"Contributing","title":"Pull Requests","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"The process described here has several goals:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Maintain quality\nEnable a sustainable system to review contributions\nEnable documented and reproducible addition of contributions","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Create a meaningful issue describing the WHY? of your contribution\nCreate a repository fork within the context of that issue.\nCreate a Draft Pull Request to the master branch of the target repository.\nDevelop, document and test your contribution (try not to solve more than one issue in a single pull request)\nAsk for merging your contribution by removing the draft marker\nIf code owners are defined, try to assign the request to a code owner","category":"page"},{"location":"contributing/#General-Objectives-1","page":"Contributing","title":"General Objectives","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"This section contains language-agnostic topics that all metal-stack projects are trying to follow.","category":"page"},{"location":"contributing/#Code-Ownership-1","page":"Contributing","title":"Code Ownership","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"The code base is owned by the entire team and every member is allowed to contribute changes to any of the projects. This is considered as collective code ownership (see https://martinfowler.com/bliki/CodeOwnership.html).","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"As a matter of fact, there are persons in a project, which already have experience with the sources. These are defined directly in the repository's CODEOWNERS file. If you want to merge changes into the master branch, it is advisable to include code owners into the proecess of discussion and merging.","category":"page"},{"location":"contributing/#Microservices-1","page":"Contributing","title":"Microservices","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"One major ambition of metal-stack is to follow the idea of microservices. This way, we want to achieve that we can","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"adapt to changes faster than with monolithic architectures,\nbe free of restrictions due to certain choices of technology,\nleverage powerful traits of cloud infrastructures (e.g. high-scalability, high-availability, ...).","category":"page"},{"location":"contributing/#Programming-Languages-1","page":"Contributing","title":"Programming Languages","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"We are generally open to write code in any language that fits best to the function of the software. However, we encourage golang to be the main language of metal-stack as we think that it makes development faster when not establishing too many different languages in our architecture. Reason for this is that we are striving for consistent behavior of the microservices, similar to what has been described for the Twelve-Factor App (see https://12factor.net/). We help enforcing unified behavior by allowing a small layer of shared code for every programming language. We will refer to this shared code as \"libraries\" for the rest of this document.","category":"page"},{"location":"contributing/#Artifacts-1","page":"Contributing","title":"Artifacts","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Artifacts are always produced by a CI process (Github Actions).","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Docker images are published on Docker Hub using the metalstack user.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Binary artifacts or images are uploaded to GKE buckets.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"When building Docker images, please consider our build tool docker-make or the specific docker-make action respectively.","category":"page"},{"location":"contributing/#APIs-1","page":"Contributing","title":"APIs","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"We are currently making use of Swagger when we exposing traditional REST APIs for end-users. This helps us with being technology-agnostic as we can generate clients in almost any language using go-swagger. Swagger additionally simplifies the documentation of our APIs.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Most APIs though are not required to be user-facing but are of technical nature. These are preferred to be implemented using grpc.","category":"page"},{"location":"contributing/#Versioning-1","page":"Contributing","title":"Versioning","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Artifacts are versioned by tagging the respective repository with a tag starting with the letter v. After the letter, there stands a valid semantic version.","category":"page"},{"location":"contributing/#Documentation-1","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"In order to make it easier for others to understand a project, we document general information and usage instructions in a README.md in any project.","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"In addition to that, we document a microservice in the docs repository. The documentation should contain the reasoning why this service exists and why it was being implemented the way it was being implemented. The aim of this procedure is to reduce the time for contributors to comprehend architectural decisions that were made during the process of writing the software and to clarify the general purpose of this service in the entire context of the software.","category":"page"},{"location":"contributing/#Guidelines-1","page":"Contributing","title":"Guidelines","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"This chapter describes general guidelines on how to develop and contribute code for a certain programming language.","category":"page"},{"location":"contributing/#Golang-1","page":"Contributing","title":"Golang","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Development follows the official guide to:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Write clear, idiomatic Go code: https://golang.org/doc/effective_go.html\nLearn from mistakes that must not be repeated: https://github.com/golang/go/wiki/CodeReviewComments\nApply appropriate names to your artifacts:\nhttps://talks.golang.org/2014/names.slide#1\nhttps://blog.golang.org/package-names\nhttps://golang.org/doc/effective_go.html#names\nEnable others to understand the reasoning of non-trivial code sequences by applying a meaningful documentation.","category":"page"},{"location":"contributing/#Development-Decisions-1","page":"Contributing","title":"Development Decisions","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Dependency Management by using Go modules\nBuild and Test Automation by using GNU Make.\nEnd-user APIs should consider using go-swagger and Go-Restful Technical APIs should consider using grpc","category":"page"},{"location":"contributing/#Libraries-1","page":"Contributing","title":"Libraries","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"metal-stack maintains several libraries that you should utilize in your project in order unify common behavior. Some of these projects are:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"metal-go\nmetal-lib","category":"page"},{"location":"contributing/#Error-Handling-with-Generated-Swagger-Clients-1","page":"Contributing","title":"Error Handling with Generated Swagger Clients","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"From the server-side you should ensure that you are returning the common error json struct in case of an error as defined in the metal-lib/httperrors. Ensure you are using go-restful >= v2.9.1 and go-restful-openapi >= v0.13.1 (allows default responses with error codes other than 200).","category":"page"},{"location":"contributing/#Documentation-2","page":"Contributing","title":"Documentation","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"We want to share knowledge and keep things simple. If things cannot kept simple we want enable everybody to understand them by:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Document in short sentences (See https://github.com/golang/go/wiki/CodeReviewComments#comment-sentences).\nDo not explain the HOW (this is already documented by your code and documenting the obvious is considered a defect).\nExplain the WHY. Add a \"to\" in your documentation line to force yourself to explain the reasonning (e.g.  \"<THE WHAT> to <THE TO>\").","category":"page"},{"location":"contributing/#Python-1","page":"Contributing","title":"Python","text":"","category":"section"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Development follows the official guide to:","category":"page"},{"location":"contributing/#","page":"Contributing","title":"Contributing","text":"Style Guide for Python Code (PEP 8): https://www.python.org/dev/peps/pep-0008/\nThe use of an IDE like PyCharm helps to write compliant code easily\nConsider setuptools for packaging\nIf you want to add a Python microservice to the mix, consider pyinstaller on Alpine to achieve small image sizes","category":"page"},{"location":"architecture/the_stack/#The-Stack-1","page":"Components","title":"The Stack","text":"","category":"section"},{"location":"external/mini-lab/README/#mini-lab-1","page":"mini-lab","title":"mini-lab","text":"","category":"section"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Small lab to setup the metal-stack locally. Starts two leaf switches and the metal-api to try metalctl and the creation of machines.","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"This requires:","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"vagrant == 2.2.9 with vagrant-libvirt plugin >= 0.1.2 for running the switch and machine VMs\ndocker and docker-compose for using containerized ansible and metalctl and helm\nkvm as hypervisor for the VMs\novmf to have a uefi firmware for virtual machines\nkind == v0.8.1 to start the metal control-plane on a kubernetes cluster v1.18.2\n(optional) haveged to have enough random entropy - only needed if the PXE process does not work","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Known limitations:","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"to keep the demo small there is no EVPN\nmachine restart and destroy does not work becaues we can not change the boot order via IPMI in the lab easily (virtual-bmc could, but it's buggy)\nlogin to the machines is only possible with virsh console","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"# Install vagrant\nwget https://releases.hashicorp.com/vagrant/2.2.7/vagrant_2.2.7_x86_64.deb\napt-get install ./vagrant_2.2.7_x86_64.deb docker.io qemu-kvm virt-manager ovmf net-tools libvirt-dev\n\n# Ensure that your user is member of the group \"libvirt\"\nusermod -G libvirt -a ${USER}\n\n# Install libvirt plugin for vagrant\nvagrant plugin install vagrant-libvirt\n\n# Install kind from https://github.com/kubernetes-sigs/kind/releases","category":"page"},{"location":"external/mini-lab/README/#Try-it-out-1","page":"mini-lab","title":"Try it out","text":"","category":"section"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Start the mini-lab with a kind cluster, a metal-api instance as well as some vagrant VMs with two leaf switches and two machine skeletons.","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"make up","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Two machines in status PXE booting are visible with metalctl machine ls","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine ls\n\nID                                          LAST EVENT   WHEN     AGE  HOSTNAME  PROJECT  SIZE          IMAGE  PARTITION\ne0ab02d2-27cd-5a5e-8efc-080ba80cf258        PXE Booting  3s\n2294c949-88f6-5390-8154-fa53d93a3313        PXE Booting  5s","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Wait until the machines reach the waiting state","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine ls\n\nID                                          LAST EVENT   WHEN     AGE  HOSTNAME  PROJECT  SIZE          IMAGE  PARTITION\ne0ab02d2-27cd-5a5e-8efc-080ba80cf258        Waiting      8s                               v1-small-x86         vagrant\n2294c949-88f6-5390-8154-fa53d93a3313        Waiting      8s                               v1-small-x86         vagrant","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Create a machine with","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"make machine","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"or the hard way with","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl network allocate \\\n        --partition vagrant \\\n        --project 00000000-0000-0000-0000-000000000000 \\\n        --name vagrant","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Lookup the network ID and run","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine create \\\n        --description test \\\n        --name machine \\\n        --hostname machine \\\n        --project 00000000-0000-0000-0000-000000000000 \\\n        --partition vagrant \\\n        --image ubuntu-19.10 \\\n        --size v1-small-x86 \\\n        --networks <network-ID>","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"See the installation process in action","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"virsh console metal_machine01/02\n...\nUbuntu 19.10 machine ttyS0\n\nmachine login:","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"One machine is now installed and has status \"Phoned Home\"","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine ls\nID                                          LAST EVENT   WHEN   AGE     HOSTNAME  PROJECT                               SIZE          IMAGE         PARTITION\ne0ab02d2-27cd-5a5e-8efc-080ba80cf258        Phoned Home  2s     21s     machine   00000000-0000-0000-0000-000000000000  v1-small-x86  Ubuntu 19.10  vagrant\n2294c949-88f6-5390-8154-fa53d93a3313        Waiting      8s                                                             v1-small-x86                vagrant","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Login with user name metal and the console password from","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine describe e0ab02d2-27cd-5a5e-8efc-080ba80cf258 | grep password\n\nconsolepassword: ...","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"To remove the kind cluster and the vagrant boxes, run","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"make cleanup","category":"page"},{"location":"external/mini-lab/README/#Reinstall-machine-1","page":"mini-lab","title":"Reinstall machine","text":"","category":"section"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Reinstall a machine with","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine reinstall \\\n        --image ubuntu-19.10 \\\n        e0ab02d2-27cd-5a5e-8efc-080ba80cf258","category":"page"},{"location":"external/mini-lab/README/#Remove-machine-1","page":"mini-lab","title":"Remove machine","text":"","category":"section"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"Remove a machine with","category":"page"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"docker-compose run metalctl machine rm e0ab02d2-27cd-5a5e-8efc-080ba80cf258","category":"page"},{"location":"external/mini-lab/README/#Development-of-metal-api,-metal-hammer-and-metal-core-1","page":"mini-lab","title":"Development of metal-api, metal-hammer and metal-core","text":"","category":"section"},{"location":"external/mini-lab/README/#","page":"mini-lab","title":"mini-lab","text":"To simplify developing changes for the metal-api, metal-hammer and metal-core, it is possible to use development artifacts from within the mini-lab. See the dev instructions for more details.","category":"page"},{"location":"architecture/networking/#Networking-1","page":"Networking","title":"Networking","text":"","category":"section"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#Dev-Instructions-1","page":"Dev Instructions","title":"Dev Instructions","text":"","category":"section"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"To simplify developing changes for the metal-api, metal-hammer and metal-core, it is possible to use development artifacts from within the mini-lab.","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Also start the mini-lab with a kind cluster, a metal-api instance as well as some vagrant VMs with two leaf switches and two machine skeletons. Additionally a Caddy and a Docker registry container is started. The former serves a prebuilt metal-hammer-initrd image, the latter holds prebuilt metalstack/metal-api and metalstack/metal-core images, which will be used as replacements for the official ones.","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Thus you have to clone the following metal-stack repositories:","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#Prerequisites:-1","page":"Dev Instructions","title":"Prerequisites:","text":"","category":"section"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"git clone https://github.com/metal-stack/metal-hammer ../metal-hammer\ngit clone https://github.com/metal-stack/metal-api ../metal-api\ngit clone https://github.com/metal-stack/metal-core ../metal-core","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#Start/Stop:-1","page":"Dev Instructions","title":"Start/Stop:","text":"","category":"section"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Build metal-hammer-initrd, metalstack/metal-api and metalstack/metal-core images and (re)start a minimal metal-stack system as well as a Caddy container that servers the former one and a Docker registry that holds the latter ones:","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"make dev","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Stop and cleanup a potentially running metal-stack development system as well as the local Caddy and Docker registry containers:","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"make down","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#Exchange-images-at-run-time:-1","page":"Dev Instructions","title":"Exchange images at run-time:","text":"","category":"section"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Reload metal-hammer-initrd:","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"make bulid-hammer-initrd","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Reload metal-api:","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"make reload-api","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"Reload metal-core:","category":"page"},{"location":"external/mini-lab/DEV_INSTRUCTIONS/#","page":"Dev Instructions","title":"Dev Instructions","text":"make reload-core","category":"page"},{"location":"proposals/#Metal-Stack-Enhancement-Proposals-(MEPs)-1","page":"Enhancement Proposals","title":"Metal Stack Enhancement Proposals (MEPs)","text":"","category":"section"},{"location":"proposals/#","page":"Enhancement Proposals","title":"Enhancement Proposals","text":"This directory contains proposals which address substantial modifications to metal-stack and are discussed here. Each Proposal is numbered and raised as a pull request.","category":"page"},{"location":"proposals/#","page":"Enhancement Proposals","title":"Enhancement Proposals","text":"Every proposal has a shortcut name which starts with MEP followed by a unique number. The proposal should be raised as a merge request, if the merge request was excepted everyone can read it. This does not necessarily mean the proposal was accepted or not. The list of proposal and their current state is listed in the table below.","category":"page"},{"location":"proposals/#","page":"Enhancement Proposals","title":"Enhancement Proposals","text":"Possible states are:","category":"page"},{"location":"proposals/#","page":"Enhancement Proposals","title":"Enhancement Proposals","text":"InDiscussion\nAccepted\nDeclined\nInProgress\nCompleted","category":"page"},{"location":"proposals/#","page":"Enhancement Proposals","title":"Enhancement Proposals","text":"Once a proposal was accepted, a issue should be raised and the implementation should be done in a seperate PR.","category":"page"},{"location":"proposals/#","page":"Enhancement Proposals","title":"Enhancement Proposals","text":"Name Description State Issue PR\nMEP-1 Distributed Control Plane Deployment InDiscussion N/A N/A\nMEP-2 Two Factor Authentication InProgress N/A N/A\nMEP-3 Machine Re-Installation to preserve local data Completed N/A N/A\nMEP-4 Multi-tenancy for the metal-api Accepted N/A N/A","category":"page"},{"location":"proposals/MEP1/README/#Distributed-Metal-Control-Plane-1","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"","category":"section"},{"location":"proposals/MEP1/README/#Problem-Statement-1","page":"Distributed Metal Control Plane","title":"Problem Statement","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"We face the situation that we argue for running bare metal on premise because this way the customers can control where and how their software and data are processed and stored. On the other hand, we have currently a hard requirement that our metal-api control plane components need to be running on a kubernetes cluster on some of the available hyperscalers. Running the control plane on kubernetes has the following benefits:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Ease of deployment\nGet most, if not all, of the required infrastructure services like (probably incomplete):\nIPs\nDNS\nL7-Loadbalancing\nStorage\nS3 Backup\nHigh Availability","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Using a kubernetes as a service offering from one of the hyperscalers (actually GKE), enables us to focus on using kubernetes instead of maintaining it as well.","category":"page"},{"location":"proposals/MEP1/README/#Goal-1","page":"Distributed Metal Control Plane","title":"Goal","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"It would be much saner if metal-stack has no, or only minimal dependencies to external services. Imagine a metal-stack deployment in a plant, it would be optimal if we only have to deliver a single rack with servers and networking gear installed and wired, plug that rack to the power supply and a internet uplink and its ready to go.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Have a second plant which you want to be part of all your plants? Just tell both that they are part of something bigger and metal-api knows of two partitions.","category":"page"},{"location":"proposals/MEP1/README/#Possible-Solutions-1","page":"Distributed Metal Control Plane","title":"Possible Solutions","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"We can think of two different solutions to this vision:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Keep the central control plane approach and require some sort of kubernetes deployment accessible from the internet. This has the downside that the user must, provide a managed kubernetes deployment in his own datacenter or uses a hyperscaler. Still not optimal.\nInstall the metal-api and all its dependencies in every partition, replicate or shard the databases to every connected partition, make them know each other. Connect the partitions over the internet with some sort of vpn to make the services visible to each other.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"As we can see, the first approach does not really address the problem, therefore i will describe solution #2 in more details.","category":"page"},{"location":"proposals/MEP1/README/#Central/Current-setup-1","page":"Distributed Metal Control Plane","title":"Central/Current setup","text":"","category":"section"},{"location":"proposals/MEP1/README/#Stateful-services-1","page":"Distributed Metal Control Plane","title":"Stateful services","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Every distributed system suffer from handling state in a scalable, fast and correct way. To start how to cope with the state, we first must identify which state can be seen as partition local only and which state must be synchronous for read, and synchronous for writes across partitions.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Affected states:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"masterdata: e.g. tenant and project must be present in every partition, but these are entities which are read often but updates are rare. A write can therefore be visible with a decent delay in a distinct partition with no consequences.\nipam: the prefixes and ip´s allocated from machines. These entities are also read often and rare updates. But we must differentiate between dirty reads for different types. A machine network is partition local, ips acquired from such a network must by synchronous in the same partition. Ips acquired from global networks such as internet must by synchronous for all partitions, as otherwise a internet ip could be acquired twice.\nvrf ids: they must only be unique in one partition\nimage and size configurations: read often, written seldom, so no high requirements on the storage of these entities.\nimages: os images are already replicated from a central s3 storage to a per partition s3 service. metal-hammer kernel and initrd are small and pull always from the central s3, can be done similar to os images.\nmachine and machine allocation: must be only synchronous in the partition\nswitch: must be only synchronous in the partition\nnsq messages: do not need to cross partition boundaries. No need to keep the messages persistent, even the opposite is true, we don't want to have the messages persist for a longer period.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Now we can see that the most critical state to held and synchronize are the IPAM data, because these entities must be guaranteed to be synchronously updated, while being updated frequently.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Datastores:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"We use three different types of datastores to persist the states of the metal application.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"rethinkdb is the main datastore for almost all entities managed by metal-api\npostgresql is used for masterdata and ipam data.\nnsq uses disk and memory tho store the messages.","category":"page"},{"location":"proposals/MEP1/README/#Stateless-services-1","page":"Distributed Metal Control Plane","title":"Stateless services","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"These are the easy part, all of our services which are stateless can be scaled up and down without any impact on functionality. Even the stateful services like masterdata and metal-api rely fully on the underlying datastore and can therefore also be scaled up and down to meet scalability requirements.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Albeit, most of these services need to be placed behind a loadbalancer which does the L4/L7 balancing across the started/available replicas of the service for the clients talking to it. This is actually provided by kubernetes with either service type loadbalancer or type clusterip.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"One exception is the metal-console service which must have the partition in it´s dns name now, because there is no direct network connectivity between the management networks of the partitions. See \"Network Setup)","category":"page"},{"location":"proposals/MEP1/README/#Distributed-setup-1","page":"Distributed Metal Control Plane","title":"Distributed setup","text":"","category":"section"},{"location":"proposals/MEP1/README/#State-1","page":"Distributed Metal Control Plane","title":"State","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"In order to replicate certain data which must be available across all partitions we can use on of the existing open source databases which enable such kind of setup. There are a few avaible out there, the following uncomplete list will highlight the pro´s and cons of each.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"RethinkDB\nWe already store most of our data in RethinkDB and it gives already the ability to synchronize the data in a distributed manner with different guarantees for consistency and latency. This is described here: Scaling, Sharding and replication. But because rethinkdb has a rough history and unsure future with the last release took more than a year, we in the team already thought that we eventually must move away from rethinkdb in the future.  \nPostgresql\nPostgres does not have a multi datacenter with replication in both directions, it just can make the remote instance store the same data.\nCockroachDB\nIs a Postgresql compatible database enginge on the wire. CockroachDB gives you both, ACID and geo replication with writes allowed from all connected members. It is even possible to configure Follow the Workload and Geo Partitioning and Replication.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"If we migrate all metal-api entities to be stored the same way we store masterdata, we could use cockroachdb to store all metal entities in one ore more databases spread across all partitions and still ensure consistency and high availability.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"A simple setup how this would look like is shown here.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"(Image: Simple CockroachDB setup)","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"go-ipam was modified in a example PR here: https://github.com/metal-pod/go-ipam/pull/17","category":"page"},{"location":"proposals/MEP1/README/#API-Access-1","page":"Distributed Metal Control Plane","title":"API Access","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"In order to make the metal-api accessible for api users like cloud-api or metalctl as easy at it is today, some effort has to be taken. One possible approach would be to use a external loadbalancer which spread the requests evenly to all metal-api endpoints in all partitions. Because all data are accessible from all partitions, a api request going to partition A with a request to create a machine in partition B, will still work. If on the other hand partition B is not in a connected state because the interconnection between both partitions is broken, then of course the request will fail.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"IMPORTANT The NSQ Message to inform metal-core must end in the correct partition","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"To provide such a external loadbalancer we have several opportunities:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Cloudflare or comparable CDN service.\nBGP Anycast from every partition","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Another setup would place a small gateway behind the metal-api address, which forwards to the metal-api in the partition where the request must be executed. This gateway, metal-api-router must inspect the payload, extract the desired partition, and forward the request without any modifications to the metal-api endpoint in this partition. This can be done for all requests, or if we want to optimize, only for write accesses.","category":"page"},{"location":"proposals/MEP1/README/#Network-setup-1","page":"Distributed Metal Control Plane","title":"Network setup","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"In order to have the impact to the overall security concept as minimal as possible i would not modify the current network setup. The only modifications which has to be made are:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Allow https ingress traffic to all metal-api instances.\nAllow ssh ingress traffic to all metal-console instances.\nAllow CockroachDB Replication between all partitions.\nNo NSQ traffic from outside required anymore, except we cant solve the topic above.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"A simple setup how this would look like is shown here, this does not work though because of the forementioned NSQ issue.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"(Image: API and Console Access)","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"Therefore we need the metal-api-router:","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"(Image: Working API and Console Access)","category":"page"},{"location":"proposals/MEP1/README/#Deployment-1","page":"Distributed Metal Control Plane","title":"Deployment","text":"","category":"section"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"The deployment of our components will substantially differ in a partition compared to a the deployment we have actually. Deploying it in kubernetes in the partition would be very difficult to achieve because we have no sane way to deploy kubernetes on physical machines without a underlying API. I would therefore suggest to deploy our components in the same way we do that for the services running on the management server. Use systemd to start docker containers.","category":"page"},{"location":"proposals/MEP1/README/#","page":"Distributed Metal Control Plane","title":"Distributed Metal Control Plane","text":"(Image: Deployment)","category":"page"},{"location":"proposals/MEP3/README/#Machine-Re-Installation-1","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"","category":"section"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"In the current metal-api only machine installations are possible, performing a machine upgrade is only possible by creating a new machine and delete the old one. This has the drawback that in case a lot of data is stored on the local disks, a full restore of the original data must be performed.","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"To prevent this, we will introduce a new metal-api endpoint to reinstall the machine with a new image, without actually deleting the data stored on the additional hard disks.","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"Storage is a difficult task to get right and reliable. A short analysis of our different storage requirements lead to 3 different scenarios.","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"Storage for the etcd pvs in the seed cluster of every partition.   This is the most important storage in our setup because these etcd pods serve as configuration backend for all customer kubernetes clusters. If they fail, the cluster is down. However gardener deploys a backup and restore sidecar into the etcd pod of every customer kubernetes control plane, and if this sidecar detects a corrupt or missing etcd database file(s) it starts automatic restore from the configured backup location. This will take some minutes. If for example a node dies, and gardener creates a new node instead, the csi-lvm created pv is not present on that node. Kubernetes will not schedule the missing etcd pod on this node because it has a local PV configured and is therefore tainted to run only on that node. To let kubernetes create that pod anyhow, someone has to either remove the taint, or delete the pod. If this is done, the pod starts and the restore of the etcd data can start as well. You can see this is a bit too complicated and will take the customer cluster down for a while (not measured yet but in the range of 5-10 minutes).\nStorage in customer clusters.   This was not promised in 2020. We have a intermediate solution with the provisioning of csi-lvm by default into all customer clusters. Albeit this is only local storage and will get deleted if a node dies.\nS3 Storage.   We have two possibilities to cope with storage:\nIn place update of the OS with a daemonset   This will be fast and simple, but might fail because the packages being installed are broken right now, or a filesystem gets full, or any other failure you can think of during a os update. Another drawback is that metal-api does not reflect the updated os image.\nmetal-api get a machine reinstall endpoint   With this approach we leverage from existing and already proven mechanisms. Reinstall must keep all data except the sata-dom. Gardener currently is not able to do an update with this approach because it can only do rolling updates. Therefore a additional osupdatestrategy has to be implemented for metal and other providers in gardener to be able to leverage the metal reinstall on the same machineID approach.","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"If reinstall is implemented, we should focus on the same technology for all scenarios and put ceph via rook.io into the kubernetes clusters as additional StorageClass. It has to be checked whether to use the raw disk or a PV as the underlay block device where ceph stores its data.","category":"page"},{"location":"proposals/MEP3/README/#API-and-behavior-1","page":"Machine Re-Installation","title":"API and behavior","text":"","category":"section"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"The API will get an new endpoint \"reinstall\" this endpoint takes two arguments:","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"machineID\nimage","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"No other aspects of the machine can be modified during the re-installation. All data stored in the existing allocation will be preserved, only the image will be modified. Once this endpoint was called, the machine will get a reboot signal with the boot order set to PXE instead of HDD and the network interfaces on the leaf are set to PXE as well. Then the normal installation process starts:","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"unchanged: PXE boot with metal-hammer\nchanged: metal-hammer first checks with the machineID in the metal-api (through metal-core) if there is already a allocation present\nchanged: if a allocation is present and the allocation has set reinstall: true, wipe disk is only executed for the root disk, all other disks are untouched.\nunchanged: the specified image is downloaded and burned, /install.sh is executed\nunchanged: successful installation is reported back, network is set the the vrf, boot order is set to HDD.\nunchanged: distribution kernel is booted via kexec","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"We can see that the allocation requires one additional parameter: reinstall and metal-hammer must check for already existing allocation at an earlier stage.","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"Components which requires modifications (first guess):","category":"page"},{"location":"proposals/MEP3/README/#","page":"Machine Re-Installation","title":"Machine Re-Installation","text":"metal-hammer:\ncheck for allocation present earlier\nevaluation of reinstall flag set\nwipe of disks depends on that flag\nBonus: move configuration of disk layout and primary disk detection algorithm (PDDA) from metal-hammer into metal-api.   metal-api MUST reject reinstallation if the disk found by PDDA does not have the /etc/metal directory!\nmetal-core:\nprobably nothing\nmetal-api:\nnew endpoint /machine/reinstall\nadd Reinstall bool to data model of allocation\nmake sure to reset Reinstall after reinstallation to prevent endless reinstallation loop\nmetalctl:\nimplement reinstall\nmetal-go:\nimplement reinstall\ngardener (longterm):\nadd the OSUpgradeStrategy reinstall","category":"page"},{"location":"#metal-stack-Documentation-1","page":"Overview","title":"metal-stack Documentation","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"metal-stack is a software that provides an API for provisioning and managing physical servers. It is a fully-automated provisioning framework for bare metal machines. To categorize this product, we commonly use the terminology metal-as-a-service (MaaS) or bare metal cloud.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Pages = [\"index.md\"]\nDepth = 5","category":"page"},{"location":"#Use-Cases-1","page":"Overview","title":"Use-Cases","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"The major intent to initiate metal-stack was to build a framework that provides an on-premise machine infrastructure for Kubernetes (K8s) as a service (KaaS). But of course you can also use metal-stack only for multi-tenant-capable machine provisioning as well in your data center.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Running on-premise has several benefits:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Full data sovereignty\nTypically better price/performance ratio than hyperscalers (especially the larger your environments are)\nEasier connectivity to existing company networks","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"metal-stack itself is typically deployed on Kubernetes as well. However, there are no specific dependencies of metal-stack running in a Kubernetes cluster. It exposes a traditional REST API that can be used for managing bare metal machines.","category":"page"},{"location":"#Kubernetes-Integration-1","page":"Overview","title":"Kubernetes Integration","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"In partnership with the open-source project Gardener, we provision Kubernetes clusters on metal-stack at scale. From the perspective of the Gardener, the metal-stack is just another cloud provider.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"We are clearly aiming at a fully-automated lifecycle for K8s clusters. The time saving compared to providing machines and Kubernetes by hand are significant. We actually want to be able to compete with offers of public cloud providers, especially regarding speed and usability.","category":"page"},{"location":"#Why-Bare-Metal?-1","page":"Overview","title":"Why Bare Metal?","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"Bare metal has several advantages over virtualized environments and overcomes several of the drawbacks of virtual machines.","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Virtual environment drawbacks\nSpectre and Meltdown can only be mitigated with a \"cluster per tenant\" approach\nMissing isolation of multi-tenant change impacts\nLicensing restrictions\n\"Noisy-neighbor\" issues\nAdvantages of a metal-as-a-service platform\nHigh and guaranteed performance (especially disk i/o)\nReduced stack depth (Host -> VM -> Application vs. Host -> Container) => reduced attack surface, cost/performance gain, no VM live-migrations\nNo need for a central storage system: Local storage governed by K8s to reduce storage costs\nBigger hardware configuration possible (hypervisors have restrictions, e.g. it is not possible to assign all CPUs to a single VM)\nK8s ships with \"enterprise\" features (performance, availability, scalability) on commodity hardware","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Beside these benefits there are also several disadvantages of metal-as-a-service platforms to consider:","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"Hardware defects have direct impact (should be considered by design) and can not be mitigated by live-migration as in virtual environments\nNot many \"providers\" to choose from\nBare metal provisioning is slower than provisioning a VM\nCapacity planning is more difficult (no resource overbooking possible)\nHigher \"AfA\"-costs","category":"page"},{"location":"#","page":"Overview","title":"Overview","text":"In the end, we have come to the conclusion that most of the drawbacks of bare metal machines can be mitigated best when running K8s on the machines. K8s will take care of high-availability in case of hardware failures and also supervises machine resources. We are certain that the chosen approach can satisfy the needs of the future users to a higher degree than virtual machines could do.","category":"page"},{"location":"#Roadmap-1","page":"Overview","title":"Roadmap","text":"","category":"section"},{"location":"#","page":"Overview","title":"Overview","text":"Coming soon.","category":"page"}]
}
